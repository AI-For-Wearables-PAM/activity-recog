{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test stratified k-fold cross validation\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      "Fold 0:\n",
      "  Train: index=[1 3]\n",
      "  Test:  index=[0 2]\n",
      "Fold 1:\n",
      "  Train: index=[0 2]\n",
      "  Test:  index=[1 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "print(skf)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify ImageNet classes with ResNet50\n",
    "https://keras.io/api/applications/#usage-examples-for-image-classification-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img_path = 'elephant.jpg'\n",
    "img = keras.utils.load_img(img_path, target_size=(224, 224))\n",
    "x = keras.utils.img_to_array(img)\n",
    "x\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "# preds = model.predict(x)\n",
    "# # decode the results into a list of tuples (class, description, probability)\n",
    "# # (one such list for each sample in the batch)\n",
    "# print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try predicing from video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def video_to_frames(video_path, img_size=(64, 64), sequence_length=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # frame = cv2.resize(frame, img_size)\n",
    "        img = keras.utils.load_img(frame, target_size=(224, 224))\n",
    "        x = keras.utils.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        frames.append(x)\n",
    "        if len(frames) == sequence_length:\n",
    "            break\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < sequence_length:\n",
    "        return None  # Ignore short videos\n",
    "\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/vision/video_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a CSV with video filenames and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# List files and ignore .DS_Store if on a Mac\n",
    "def list_files(directory):\n",
    "    visible_files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if not file.startswith('.'):\n",
    "            visible_files.append(file)\n",
    "\n",
    "    return visible_files\n",
    "\n",
    "def catalog_videos(folder_path):\n",
    "    # classes = os.listdir(folder_path)\n",
    "    classes = list_files(folder_path)\n",
    "    videos, labels, encoded_labels, paths = [], [], [], []\n",
    "\n",
    "    for label, activity in enumerate(classes):\n",
    "        activity_folder = os.path.join(folder_path, activity)\n",
    "        # for video_file in os.listdir(activity_folder):\n",
    "        for video_file in list_files(activity_folder):\n",
    "            video_path = os.path.join(activity_folder, video_file)\n",
    "            videos.append(video_file)\n",
    "            labels.append(activity)\n",
    "            encoded_labels.append(label)\n",
    "            paths.append(video_path)\n",
    "\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    le.transform(labels)\n",
    "\n",
    "    return videos, labels, encoded_labels, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_catalog_csv(path, name):\n",
    "    # Create catalog of dataset details\n",
    "    catalog = catalog_videos(path)\n",
    "\n",
    "    # Make dataframe\n",
    "    df = pd.DataFrame({'video': catalog[0], 'label': catalog[1],\n",
    "                            'encoded_label': catalog[2], 'path': catalog[3]})\n",
    "    # Export CSV\n",
    "    filename = f'{name}.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    print(f'Saved to {filename}')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to train_df.csv\n",
      "Saved to test_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Make CSV for training/testing sets\n",
    "train_path = \"../../downloads/old_clips/full_res/train\"\n",
    "test_path = \"../../downloads/old_clips/full_res/test\"\n",
    "\n",
    "train_df = make_catalog_csv(train_path, \"train_df\")\n",
    "test_df = make_catalog_csv(test_path, \"test_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>label</th>\n",
       "      <th>encoded_label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7394377634358283409.mp4</td>\n",
       "      <td>EVS Visit</td>\n",
       "      <td>0</td>\n",
       "      <td>../../downloads/old_clips/full_res/test/EVS Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7394376363047963793.mp4</td>\n",
       "      <td>EVS Visit</td>\n",
       "      <td>0</td>\n",
       "      <td>../../downloads/old_clips/full_res/test/EVS Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7394375134687317137.mp4</td>\n",
       "      <td>EVS Visit</td>\n",
       "      <td>0</td>\n",
       "      <td>../../downloads/old_clips/full_res/test/EVS Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7394377110372273297.mp4</td>\n",
       "      <td>EVS Visit</td>\n",
       "      <td>0</td>\n",
       "      <td>../../downloads/old_clips/full_res/test/EVS Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7394376070990187665.mp4</td>\n",
       "      <td>EVS Visit</td>\n",
       "      <td>0</td>\n",
       "      <td>../../downloads/old_clips/full_res/test/EVS Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>7395538125931728017.mp4</td>\n",
       "      <td>Nurse Visit</td>\n",
       "      <td>10</td>\n",
       "      <td>../../downloads/old_clips/full_res/test/Nurse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>7395888535133539473.mp4</td>\n",
       "      <td>Nurse Visit</td>\n",
       "      <td>10</td>\n",
       "      <td>../../downloads/old_clips/full_res/test/Nurse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>7394438386670685329.mp4</td>\n",
       "      <td>Nurse Visit</td>\n",
       "      <td>10</td>\n",
       "      <td>../../downloads/old_clips/full_res/test/Nurse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>7393435193979489425.mp4</td>\n",
       "      <td>Transfer To Bed</td>\n",
       "      <td>11</td>\n",
       "      <td>../../downloads/old_clips/full_res/test/Transf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>7393687927035055249.mp4</td>\n",
       "      <td>Transfer To Bed</td>\n",
       "      <td>11</td>\n",
       "      <td>../../downloads/old_clips/full_res/test/Transf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>446 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       video            label  encoded_label  \\\n",
       "0    7394377634358283409.mp4        EVS Visit              0   \n",
       "1    7394376363047963793.mp4        EVS Visit              0   \n",
       "2    7394375134687317137.mp4        EVS Visit              0   \n",
       "3    7394377110372273297.mp4        EVS Visit              0   \n",
       "4    7394376070990187665.mp4        EVS Visit              0   \n",
       "..                       ...              ...            ...   \n",
       "441  7395538125931728017.mp4      Nurse Visit             10   \n",
       "442  7395888535133539473.mp4      Nurse Visit             10   \n",
       "443  7394438386670685329.mp4      Nurse Visit             10   \n",
       "444  7393435193979489425.mp4  Transfer To Bed             11   \n",
       "445  7393687927035055249.mp4  Transfer To Bed             11   \n",
       "\n",
       "                                                  path  \n",
       "0    ../../downloads/old_clips/full_res/test/EVS Vi...  \n",
       "1    ../../downloads/old_clips/full_res/test/EVS Vi...  \n",
       "2    ../../downloads/old_clips/full_res/test/EVS Vi...  \n",
       "3    ../../downloads/old_clips/full_res/test/EVS Vi...  \n",
       "4    ../../downloads/old_clips/full_res/test/EVS Vi...  \n",
       "..                                                 ...  \n",
       "441  ../../downloads/old_clips/full_res/test/Nurse ...  \n",
       "442  ../../downloads/old_clips/full_res/test/Nurse ...  \n",
       "443  ../../downloads/old_clips/full_res/test/Nurse ...  \n",
       "444  ../../downloads/old_clips/full_res/test/Transf...  \n",
       "445  ../../downloads/old_clips/full_res/test/Transf...  \n",
       "\n",
       "[446 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asleep-Trying to sleep', 'Doctor Visit', 'EVS Visit', 'Eating', 'Family', 'Lying In Bed', 'Nurse Visit', 'Sitting In Wheelchair', 'Talking on the Phone', 'Therapy', 'Transfer To Bed', 'Watching TV']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"label\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"path\"].values.tolist()\n",
    "    labels = df[\"label\"].values\n",
    "    labels = keras.ops.convert_to_numpy(label_processor(labels[..., None]))\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        # frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = load_video(path)\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(\n",
    "            shape=(\n",
    "                1,\n",
    "                MAX_SEQ_LENGTH,\n",
    "            ),\n",
    "            dtype=\"bool\",\n",
    "        )\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :], verbose=0,\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "# Use paths declared earlier\n",
    "train_path = \"../../downloads/old_clips/full_res/train\"\n",
    "test_path = \"../../downloads/old_clips/full_res/test\"\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, train_path)\n",
    "test_data, test_labels = prepare_all_videos(test_df, test_path)\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    # x = keras.layers.Dense(8, activation=\"softmax\")(x)\n",
    "    x = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"sigmoid\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "    \n",
    "    # opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "        # loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifier/ckpt.weights.h5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 2.5246\n",
      "Epoch 1: val_loss improved from inf to 2.50697, saving model to /tmp/video_classifier/ckpt.weights.h5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.0050 - loss: 2.5089 - val_accuracy: 0.0000e+00 - val_loss: 2.5070\n",
      "Epoch 2/10\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0469 - loss: 2.4935\n",
      "Epoch 2: val_loss improved from 2.50697 to 2.50667, saving model to /tmp/video_classifier/ckpt.weights.h5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0341 - loss: 2.4870 - val_accuracy: 0.0000e+00 - val_loss: 2.5067\n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0700 - loss: 2.4810\n",
      "Epoch 3: val_loss did not improve from 2.50667\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0722 - loss: 2.4808 - val_accuracy: 0.0000e+00 - val_loss: 2.5069\n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0888 - loss: 2.4714\n",
      "Epoch 4: val_loss did not improve from 2.50667\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0916 - loss: 2.4713 - val_accuracy: 0.0000e+00 - val_loss: 2.5071\n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.1767 - loss: 2.4619\n",
      "Epoch 5: val_loss did not improve from 2.50667\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1760 - loss: 2.4619 - val_accuracy: 0.0000e+00 - val_loss: 2.5077\n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2318 - loss: 2.4577\n",
      "Epoch 6: val_loss did not improve from 2.50667\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.2320 - loss: 2.4574 - val_accuracy: 0.0000e+00 - val_loss: 2.5087\n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2415 - loss: 2.4562\n",
      "Epoch 7: val_loss did not improve from 2.50667\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2428 - loss: 2.4554 - val_accuracy: 0.0963 - val_loss: 2.5094\n",
      "Epoch 8/10\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2581 - loss: 2.4467\n",
      "Epoch 8: val_loss did not improve from 2.50667\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2594 - loss: 2.4453 - val_accuracy: 0.0963 - val_loss: 2.5102\n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3025 - loss: 2.4362\n",
      "Epoch 9: val_loss did not improve from 2.50667\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3021 - loss: 2.4360 - val_accuracy: 0.0963 - val_loss: 2.5113\n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3186 - loss: 2.4310\n",
      "Epoch 10: val_loss did not improve from 2.50667\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.3235 - loss: 2.4306 - val_accuracy: 0.0963 - val_loss: 2.5124\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0048 - loss: 2.4768   \n",
      "Test accuracy: 0.22%\n"
     ]
    }
   ],
   "source": [
    "_, sequence_model = run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: ../../downloads/test/Eating/7395553781087521937.mp4\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "  Doctor Visit: 56.92%\n",
      "  Sitting In Wheelchair: 53.53%\n",
      "  Transfer To Bed: 50.68%\n",
      "  Talking on the Phone: 49.06%\n",
      "  EVS Visit: 47.47%\n",
      "  Asleep-Trying to sleep: 47.40%\n",
      "  Nurse Visit: 46.25%\n",
      "  Watching TV: 43.84%\n",
      "  Family: 43.70%\n",
      "  Lying In Bed: 41.84%\n",
      "  Eating: 41.83%\n",
      "  Therapy: 41.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(<unknown>:65618): GStreamer-CRITICAL **: 15:40:02.276: gst_element_make_from_uri: assertion 'gst_uri_is_valid (uri)' failed\n",
      "[ WARN:0@36291.277] global cap_gstreamer.cpp:1436 open OpenCV | GStreamer warning: Error opening bin: no element \"test\"\n",
      "[ WARN:0@36291.277] global cap_gstreamer.cpp:1173 isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "OpenCV: Couldn't read video stream from file \"test/../../downloads/test/Eating/7395553781087521937.mp4\"\n",
      "[ WARN:0@36291.280] global cap.cpp:166 open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.10.0) /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_49s_p64pd6/croot/opencv-suite_1722029132360/work/modules/videoio/src/cap_images.cpp:274: error: (-215:Assertion failed) number < max_number in function 'icvExtractPattern'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(\n",
    "        shape=(\n",
    "            1,\n",
    "            MAX_SEQ_LENGTH,\n",
    "        ),\n",
    "        dtype=\"bool\",\n",
    "    )\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "\n",
    "# This utility is for visualization.\n",
    "# Referenced from:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def to_gif(images):\n",
    "    converted_images = images.astype(np.uint8)\n",
    "    imageio.mimsave(\"animation.gif\", converted_images, duration=100)\n",
    "    return Image(\"animation.gif\")\n",
    "\n",
    "\n",
    "# train_path = f\"../../downloads/train\"\n",
    "# test_path = f\"../../downloads/test\"\n",
    "\n",
    "# train_data, train_labels = prepare_all_videos(train_df, train_path)\n",
    "# test_data, test_labels = prepare_all_videos(test_df, test_path)\n",
    "\n",
    "test_video = np.random.choice(test_df[\"path\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)\n",
    "# to_gif(test_frames[:MAX_SEQ_LENGTH])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
